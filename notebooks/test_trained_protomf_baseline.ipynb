{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There is 5% Female and 18% Male in the artists -> data is shit!\n",
    "## There is ~60% correct country formatting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dataset_path = '/data/lfm2b-1mon'\n",
    "\n",
    "users = pd.read_csv(os.path.join(dataset_path, 'users.tsv'), sep='\\t')\n",
    "users.rename(columns={'user_id': 'old_user_id'}, inplace=True)\n",
    "\n",
    "test_dataset_path = dataset_path + '/listening_history_test.csv'\n",
    "test_data = pd.read_csv(test_dataset_path)\n",
    "test_data_user_augmented = test_data.merge(users, on='old_user_id')\n",
    "test_data_user_augmented.head()\n",
    "print(test_data_user_augmented.head())\n",
    "\n",
    "tracks = pd.read_csv(os.path.join(dataset_path, 'tracks_augmented.csv'))\n",
    "tracks.rename(columns={'track_id': 'old_item_id'}, inplace=True)\n",
    "tracks.head()\n",
    "\n",
    "test_data_user_item_augmented = test_data_user_augmented.merge(tracks, on='old_item_id')\n",
    "test_data_user_item_augmented.head()\n",
    "\n",
    "print('TRACKS\\n\\n\\n', tracks)\n",
    "\n",
    "\n",
    "test_data_user_item_augmented.rename(columns=\n",
    "                                     {'country_x': 'user_country', 'age':'user_age', 'gender_x': 'user_gender', 'artist': 'artist_name', 'track':'track_name', 'gender_y':'artist_gender', 'country_y':'artist_country'}, inplace=True)\n",
    "test_data_user_item_augmented.head()\n",
    "\n",
    "train_dataset = pd.read_csv(os.path.join(dataset_path, 'listening_history_train.csv'))\n",
    "train_dataset.head()\n",
    "\n",
    "test_data_user_item_augmented['user_num_interactions'] = train_dataset.groupby('user_id').size()[test_data_user_item_augmented['user_id']].values\n",
    "test_data_user_item_augmented['item_num_interactions'] = train_dataset.groupby('item_id').size()[test_data_user_item_augmented['item_id']].values\n",
    "\n",
    "train_dataset_track_augmented = train_dataset.merge(tracks, on='old_item_id')\n",
    "\n",
    "train_dataset_track_augmented['user_num_interactions'] = train_dataset.groupby('user_id').size()[train_dataset_track_augmented['user_id']].values\n",
    "train_dataset_track_augmented['item_num_interactions'] = train_dataset.groupby('item_id').size()[train_dataset_track_augmented['item_id']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_track_augmented['item_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from rec_sys.tester import Tester\n",
    "from experiment_helper import load_data\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "\n",
    "models = {\"b\": \"-\", \"kl\": \"-\", \"zs\": \"-\"}\n",
    "\n",
    "for i, k in enumerate(models.keys()):\n",
    "    path_to_experiment = model_paths[i]\n",
    "    config_path = os.path.join(path_to_experiment, 'params.json')\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    config['device'] = 'cpu'\n",
    "    config_dict = config.copy()\n",
    "    config = argparse.Namespace(**config)\n",
    "    data_loaders_dict = load_data(config, is_train=False)\n",
    "\n",
    "    model_load_path = os.path.join(model_paths[i], 'checkpoint_000000/best_model.pth')\n",
    "    tester = Tester(data_loaders_dict['test_loader'], config, model_load_path)\n",
    "    models[k] = tester.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from rec_sys.tester import Tester\n",
    "from experiment_helper import load_data\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from utilities.eval import Evaluator\n",
    "import experiment_helper\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tester = Tester(data_loaders_dict['test_loader'], config, model_load_path)\n",
    "test_loader = experiment_helper.load_data(config, is_train=False)['test_loader']\n",
    "\n",
    "\n",
    "def get_outputs_for_eval(model):   \n",
    "    outs_array_items = []\n",
    "    outs_array_users = []\n",
    "\n",
    "    for u_idxs, i_idxs, labels in test_loader:    \n",
    "        test_loss = 0\n",
    "        eval = Evaluator(u_idxs)\n",
    "        out = model(u_idxs, i_idxs).detach()\n",
    "        test_loss += model.loss_func(out, labels).item()\n",
    "        out = nn.Sigmoid()(out)\n",
    "        out = out.to('cpu')\n",
    "        \n",
    "        outs_array_items.append(np.array(torch.stack((i_idxs, labels, out), dim=2)))\n",
    "        outs_array_users.append(np.array(u_idxs))\n",
    "    return outs_array_items, outs_array_users\n",
    "\n",
    "\n",
    "\n",
    "def get_processed_outputs(outs_array_items):\n",
    "    pop_rank_tuples = []\n",
    "    countries = {'US': [], 'GB': [], 'BR': [], 'NL': [], 'PL': [], 'AT': [], 'JP': [], 'CA': []}  # AT: austria , JP: japan, CA: canada, US: us, GB: gb, FR: france, RU: russia, IT: Italy\n",
    "    genders = {'Male':[], 'Female':[]}\n",
    "    for epoch_recom_list in tqdm(outs_array_items, desc=\"Processing epochs\"):\n",
    "        for i, users_recom_list in enumerate(tqdm(epoch_recom_list, desc=\"Processing users\", leave=False)):\n",
    "            sorted_indices = np.argsort(users_recom_list[:, 2])[::-1]\n",
    "            ranked_outputs = np.empty_like(users_recom_list[:, 2])\n",
    "            ranked_outputs[sorted_indices] = np.arange(0, len(users_recom_list[:, 2]))            \n",
    "            users_recom_list = np.column_stack((users_recom_list, ranked_outputs.reshape(-1, 1)))\n",
    "                        \n",
    "            for item in users_recom_list:\n",
    "                item_id = int(item[0])\n",
    "                _ = item[1]\n",
    "                item_output = item[2]\n",
    "                item_rank = int(item[3])\n",
    "                \n",
    "                try:\n",
    "                    item_pop = train_dataset_track_augmented[train_dataset_track_augmented['item_id'] == item_id]['item_num_interactions'].iloc[0]                    \n",
    "                    pop_rank_tuples.append((item_id, item_pop, item_rank))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    item_country = train_dataset_track_augmented[train_dataset_track_augmented['item_id'] == item_id]['country'].iloc[0]\n",
    "                    if item_country in list(countries.keys()):\n",
    "                        countries[item_country].append(item_rank)\n",
    "                except:\n",
    "                    pass\n",
    "                                    \n",
    "                try:\n",
    "                    item_gender = train_dataset_track_augmented[train_dataset_track_augmented['item_id'] == item_id]['gender'].iloc[0]\n",
    "                    if item_gender in list(genders.keys()):\n",
    "                        genders[item_gender].append(item_rank)\n",
    "                except:\n",
    "                    pass\n",
    "        break\n",
    "        \n",
    "    return countries, genders, pop_rank_tuples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = {}\n",
    "\n",
    "for i, model_path in enumerate(model_paths):\n",
    "    print(i)\n",
    "    path_to_experiment = model_path\n",
    "    config_path = os.path.join(path_to_experiment, 'params.json')\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    config['device'] = 'cpu'\n",
    "    config_dict = config.copy()\n",
    "    config = argparse.Namespace(**config)\n",
    "    data_loaders_dict = load_data(config, is_train=False)\n",
    "\n",
    "    model_load_path = os.path.join(model_path, 'checkpoint_000000/best_model.pth')\n",
    "    tester = Tester(data_loaders_dict['test_loader'], config, model_load_path)\n",
    "    model = tester.model\n",
    "\n",
    "    test_loader = experiment_helper.load_data(config, is_train=False)['test_loader']\n",
    "    print(f'model loaded / {i}')\n",
    "    outs_model_items, outs_base_users = get_outputs_for_eval(model)\n",
    "    print(f'got outputs / {i}')\n",
    "    model_outputs_processed = get_processed_outputs(outs_model_items)\n",
    "    model_outputs[i] = model_outputs_processed\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_sorted_tuples = sorted(model_outputs[0][2], key=lambda x: x[0], reverse=False) # ascending popularity\n",
    "cont_sorted_tuples = sorted(model_outputs[1][2], key=lambda x: x[0], reverse=False) # ascending popularity\n",
    "zs_sorted_tuples = sorted(model_outputs[2][2], key=lambda x: x[0], reverse=False) # ascending popularity\n",
    "\n",
    "percentiles = np.arange(0.1, 1.0, 0.1)\n",
    "avg_ranks_b = []\n",
    "avg_ranks_c = []\n",
    "avg_ranks_z = []\n",
    "for percentile in percentiles:\n",
    "    below_p = int((percentile-.1) * len(cont_sorted_tuples))\n",
    "    above_p = int(percentile * len(cont_sorted_tuples))\n",
    "    print(below_p, above_p)\n",
    "    \n",
    "    average_rank_b = np.mean([t[1] for t in base_sorted_tuples[below_p:above_p]])\n",
    "    average_rank_c = np.mean([t[1] for t in cont_sorted_tuples[below_p:above_p]])\n",
    "    average_rank_z = np.mean([t[1] for t in zs_sorted_tuples[below_p:above_p]])\n",
    "\n",
    "    avg_ranks_b.append(average_rank_b)\n",
    "    avg_ranks_c.append(average_rank_c)\n",
    "    avg_ranks_z.append(average_rank_z)\n",
    "    \n",
    "plt.xlabel('percentiles of popularity')    \n",
    "plt.ylabel('avreage rank') \n",
    "plt.plot(percentiles, avg_ranks_b, label='baseline')\n",
    "plt.plot(percentiles, avg_ranks_c, label='our contribution')\n",
    "plt.plot(percentiles, avg_ranks_z, label='zs contribution')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_ranks_per_country = {'baseline':model_outputs[0][0], 'kl': model_outputs[1][0], 'zs':model_outputs[2][0]}\n",
    "\n",
    "averages = {}\n",
    "for element, countries in model_output_ranks_per_country.items():\n",
    "    averages[element] = {country: (sum(values) / len(values) if values else 0) for country, values in countries.items()}\n",
    "\n",
    "\n",
    "# save averages dictionary to a file\n",
    "\n",
    "import json\n",
    "\n",
    "with open('averages_2.json', 'w') as f:\n",
    "    json.dump(averages, f)\n",
    "    \n",
    "\n",
    "print(averages)\n",
    "# Prepare data for plotting\n",
    "countries = sorted({country for element in averages for country in averages[element]})\n",
    "elements = sorted(averages.keys())\n",
    "\n",
    "# Plotting\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(countries))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "elements[1], elements[2] = elements[2], elements[1]\n",
    "for i, element in enumerate(elements):\n",
    "    print(element)\n",
    "    values = [averages[element][country] for country in countries]\n",
    "    bar_positions = index + i * bar_width - 0.11\n",
    "    color = 'blue' if i == 0 else 'green' if i == 1 else 'grey' # Color for US and GB\n",
    "    ax.bar(bar_positions, values, bar_width, label=element, color=color)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Country')\n",
    "ax.set_ylabel('Average Value')\n",
    "ax.set_title('Average Values of Elements per Country')\n",
    "ax.set_xticks(index + bar_width / len(elements))\n",
    "ax.set_xticklabels(countries)\n",
    "ax.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "with open('averages_1.json', 'r') as f:\n",
    "    averages = json.load(f)\n",
    "\n",
    "# Groups\n",
    "overrepresented = ['US', 'GB', 'CA']\n",
    "underrepresented = ['BR', 'NL', 'PL']\n",
    "\n",
    "# Labels and data preparation\n",
    "labels = ['baseline', 'zs', 'kl']\n",
    "\n",
    "# Data preparation for plotting\n",
    "over_data = {label: [averages[label][country] for country in overrepresented] for label in labels}\n",
    "under_data = {label: [averages[label][country] for country in underrepresented] for label in labels}\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "x = np.arange(len(overrepresented))\n",
    "width = 0.2\n",
    "\n",
    "# Overrepresented\n",
    "for i, label in enumerate(labels):\n",
    "    axs[0].bar(x + i * width, over_data[label], width, label=label)\n",
    "\n",
    "axs[0].set_title('Overrepresented (US, GB, CA)')\n",
    "axs[0].set_xticks(x + width)\n",
    "axs[0].set_xticklabels(overrepresented)\n",
    "axs[0].legend(title='Models')\n",
    "\n",
    "# Underrepresented\n",
    "x = np.arange(len(underrepresented))\n",
    "for i, label in enumerate(labels):\n",
    "    axs[1].bar(x + i * width, under_data[label], width, label=label)\n",
    "\n",
    "axs[1].set_title('Underrepresented (BR, NL, PL)')\n",
    "axs[1].set_xticks(x + width)\n",
    "axs[1].set_xticklabels(underrepresented)\n",
    "axs[1].legend(title='Models')\n",
    "\n",
    "# General layout\n",
    "plt.suptitle('Averages Comparison Across Different Groups and Methods')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "with open('averages_1.json', 'r') as f:\n",
    "    averages = json.load(f)\n",
    "\n",
    "# Groups\n",
    "overrepresented = ['US', 'GB', 'CA']\n",
    "underrepresented = ['BR', 'NL', 'PL']\n",
    "\n",
    "# Labels and data preparation\n",
    "labels = ['baseline', 'zs', 'kl']\n",
    "aaa = {'baseline': 'Baseline', 'zs': 'ZeroSum', 'kl': 'Our Model'}\n",
    "\n",
    "\n",
    "# Data preparation for plotting\n",
    "over_data = {label: [averages[label][country] for country in overrepresented] for label in labels}\n",
    "under_data = {label: [averages[label][country] for country in underrepresented] for label in labels}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "x = np.arange(len(overrepresented))\n",
    "width = 0.2\n",
    "\n",
    "# Overrepresented\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Blue, Orange, Green\n",
    "for i, label in enumerate(labels):\n",
    "    axs[0].bar(x + i * width, over_data[label], width, label=aaa[label], color=colors[i])\n",
    "\n",
    "axs[0].set_title('Overrepresented (The US, The UK, Canada)', fontsize=16)\n",
    "axs[0].set_xticks(x + width * (len(labels) - 1) / 2)\n",
    "axs[0].set_xticklabels(overrepresented, fontsize=18)\n",
    "axs[0].set_ylabel('Average Rank of Items', fontsize=18)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[0].legend(title='Models', loc='lower right', fontsize=18)\n",
    "\n",
    "# Underrepresented\n",
    "x = np.arange(len(underrepresented))\n",
    "for i, label in enumerate(labels):\n",
    "    axs[1].bar(x + i * width, under_data[label], width, label=aaa[label], color=colors[i])\n",
    "\n",
    "axs[1].set_title('Underrepresented (Bazil, Netherlands, Poland)', fontsize=16)\n",
    "axs[1].set_xticks(x + width * (len(labels) - 1) / 2)\n",
    "axs[1].set_xticklabels(underrepresented, fontsize=18)\n",
    "axs[1].grid(True, linestyle='--', alpha=0.7)\n",
    "axs[1].legend(title='Models', loc='lower right', fontsize=18)\n",
    "\n",
    "# General layout\n",
    "# plt.suptitle('Averages Comparison Across Different Groups and Methods', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre plotting, just metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save averages dictionary to a file\n",
    "\n",
    "import json\n",
    "\n",
    "with open('averages_0.json', 'w') as f:\n",
    "    json.dump(averages, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_model(name):\n",
    "    model = models[name]\n",
    "    global user_embeddings, user_prototypes, item_embeddings, item_prototypes\n",
    "    global user_prototype_model, item_prototype_model\n",
    "    user_prototype_model = model.user_feature_extractor.model_1\n",
    "    item_prototype_model = model.item_feature_extractor.model_1\n",
    "    user_embeddings = user_prototype_model.embedding_ext.embedding_layer.weight[:]\n",
    "    user_prototypes = user_prototype_model.prototypes\n",
    "    item_embeddings = item_prototype_model.embedding_ext.embedding_layer.weight[:]\n",
    "    item_prototypes = item_prototype_model.prototypes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "    \n",
    "def distance_to_prototypes(indices, prototypes, embeddings, mode='average', k=5):\n",
    "\n",
    "    def average_distance_to_prototypes(idx):\n",
    "        cosine_similarity = F.cosine_similarity(embeddings[idx].unsqueeze(0), prototypes, dim=1)\n",
    "        return 1 - cosine_similarity.mean().item()\n",
    "\n",
    "    def distance_to_closest_prototype(idx):\n",
    "        closest_prototype_index = F.cosine_similarity(embeddings[idx].unsqueeze(0), prototypes, dim=1).argmax().item()\n",
    "        cosine_similarity = F.cosine_similarity(embeddings[idx].unsqueeze(0), prototypes[closest_prototype_index], dim=1)\n",
    "        return 1 - cosine_similarity.mean().item() \n",
    "        \n",
    "    def distance_to_top_k_closest_prototypes(idx):\n",
    "        closest_prototype_indices = F.cosine_similarity(embeddings[idx].unsqueeze(0), prototypes, dim=1).topk(k).indices\n",
    "        cosine_similarity = F.cosine_similarity(embeddings[idx].unsqueeze(0), prototypes[closest_prototype_indices], dim=1)\n",
    "        return 1 - cosine_similarity.mean().item()\n",
    "    \n",
    "    if mode == 'average':\n",
    "        distance_function = average_distance_to_prototypes\n",
    "    elif mode == 'closest':\n",
    "        distance_function = distance_to_closest_prototype\n",
    "    elif mode == 'top_k':\n",
    "        distance_function = distance_to_top_k_closest_prototypes\n",
    "\n",
    "    average_distances = []\n",
    "    for idx in indices:\n",
    "        average_distances.append(distance_function(idx))\n",
    "    return np.array(average_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## average distance to prototypes accross genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_user_ids = test_data_user_item_augmented.loc[test_data_user_item_augmented['user_gender']=='m']['user_id']\n",
    "female_user_ids = test_data_user_item_augmented.loc[test_data_user_item_augmented['user_gender']=='f']['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ks = np.arange(1, user_prototypes.shape[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_distances = []\n",
    "female_distances = []\n",
    "for k in user_ks:\n",
    "    male_distances.append(distance_to_prototypes(male_user_ids, user_prototypes, user_embeddings, mode='top_k', k=k).mean())\n",
    "    female_distances.append(distance_to_prototypes(female_user_ids, user_prototypes, user_embeddings, mode='top_k', k=k).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(user_ks, male_distances, 'b', label=f'Male Distances to k nearest prototypes, trained with k = {-1}')\n",
    "plt.plot(user_ks, female_distances, 'r', label=f'Female Distances to k nearest prototypes, trained with k = {-1}')\n",
    "plt.legend()\n",
    "\n",
    "# locally save the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = test_data_user_item_augmented.groupby(['user_country']).size().sort_values(ascending=False)\n",
    "\n",
    "pop_countries = list(country_counts[:3].index) \n",
    "mid_countries = list(country_counts[90:93].index)\n",
    "\n",
    "pop_countries_ids = test_data_user_item_augmented[test_data_user_item_augmented['user_country'].isin(['IR'])]['user_id']\n",
    "mid_countries_ids = test_data_user_item_augmented[test_data_user_item_augmented['user_country'].isin(mid_countries)]['user_id']\n",
    "\n",
    "pop_distances = []\n",
    "mid_distances = []\n",
    "for k in user_ks:\n",
    "    pop_distances.append(distance_to_prototypes(pop_countries_ids, user_prototypes, user_embeddings, mode='top_k', k=k).mean())\n",
    "    mid_distances.append(distance_to_prototypes(mid_countries_ids, user_prototypes, user_embeddings, mode='top_k', k=k).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = test_data_user_item_augmented.groupby(['user_country']).size().sort_values(ascending=False)\n",
    "\n",
    "pop_countries = list(country_counts[:3].index) \n",
    "mid_countries = list(country_counts[90:93].index)\n",
    "\n",
    "pop_countries_ids = test_data_user_item_augmented[test_data_user_item_augmented['user_country'].isin(pop_countries)]['user_id']\n",
    "mid_countries_ids = test_data_user_item_augmented[test_data_user_item_augmented['user_country'].isin(mid_countries)]['user_id']\n",
    "\n",
    "pop_distances = []\n",
    "mid_distances = []\n",
    "for k in user_ks:\n",
    "    pop_distances.append(distance_to_prototypes(pop_countries_ids, user_prototypes, user_embeddings, mode='top_k', k=k).mean())\n",
    "    mid_distances.append(distance_to_prototypes(mid_countries_ids, user_prototypes, user_embeddings, mode='top_k', k=k).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(user_ks, pop_distances, 'b', label=f'Pop Distances to k nearest prototypes, trained with k = {-1}')\n",
    "plt.plot(user_ks, mid_distances, 'r', label=f'Mid Distances to k nearest prototypes, trained with k = {-1}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented['user_num_interactions'].describe(percentiles=[0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_arr_popularity_groups = []\n",
    "\n",
    "percentiles = {25: 57, 50: 130, 75: 245, 90: 395, 99: 749, 100: 1347}\n",
    "per = [0] + list(percentiles.values())\n",
    "percentilesss = list(percentiles.keys())\n",
    "for i in range(len(per) - 1):\n",
    "    user_ids_arr_popularity_groups.append(test_data_user_item_augmented.loc[(test_data_user_item_augmented['user_num_interactions'] > per[i]) & (test_data_user_item_augmented['user_num_interactions'] < per[i + 1])]['user_id'])\n",
    "\n",
    "distances_arr = [[] for _ in range(len(user_ids_arr_popularity_groups))]\n",
    "for user_group in range(len(user_ids_arr_popularity_groups)):\n",
    "    for k in user_ks:\n",
    "        distances_arr[user_group].append(distance_to_prototypes(user_ids_arr_popularity_groups[user_group], user_prototypes, user_embeddings, mode='top_k', k=k).mean())\n",
    "distances_arr = np.array(distances_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "\n",
    "\n",
    "for i in range(len(user_ids_arr_popularity_groups)):\n",
    "    plt.plot(user_ks, distances_arr[i]**5, label=f'Popularity Group Percentile - {percentilesss[i]}: {percentiles[percentilesss[i]]} Interactions, trained with k = {1}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented['item_num_interactions'].describe(percentiles=[0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITEM SIDE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_model('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ks = np.arange(1, item_prototypes.shape[0], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented['artist_gender'].value_counts(), test_data_user_item_augmented['artist_gender'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_item_ids = test_data_user_item_augmented.loc[test_data_user_item_augmented['artist_gender']=='Male']['item_id']\n",
    "female_item_ids = test_data_user_item_augmented.loc[test_data_user_item_augmented['artist_gender']=='Female']['item_id']\n",
    "\n",
    "male_distances = []\n",
    "female_distances = []\n",
    "for k in item_ks:\n",
    "    male_distances.append(distance_to_prototypes(male_item_ids, item_prototypes, item_embeddings, mode='top_k', k=k).mean())\n",
    "    female_distances.append(distance_to_prototypes(female_item_ids, item_prototypes, item_embeddings, mode='top_k', k=k).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(item_ks, male_distances, 'b', label=f'Male Distances to k nearest prototypes, trained with k = {1}')\n",
    "plt.plot(item_ks, female_distances, 'r', label=f'Female Distances to k nearest prototypes, trained with k = {1}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_track_augmented['item_num_interactions'].describe(percentiles=[0.1, 0.25, 0.3, 0.4, 0.5, 0.6, 0.75, 0.8, 0.9, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = []\n",
    "\n",
    "models = {\"b\": \"-\"}\n",
    "\n",
    "for i, k in enumerate(models.keys()):\n",
    "    path_to_experiment = model_paths[i]\n",
    "    config_path = os.path.join(path_to_experiment, 'params.json')\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    config['device'] = 'cpu'\n",
    "    config_dict = config.copy()\n",
    "    config = argparse.Namespace(**config)\n",
    "    data_loaders_dict = load_data(config, is_train=False)\n",
    "\n",
    "    model_load_path = os.path.join(model_paths[i], 'checkpoint_000000/best_model.pth')\n",
    "    tester = Tester(data_loaders_dict['test_loader'], config, model_load_path)\n",
    "    models[k] = tester.model\n",
    "\n",
    "change_model('b')\n",
    "\n",
    "\n",
    "item_ks = np.arange(1, item_prototypes.shape[0], 5)\n",
    "item_ids_arr_popularity_groups = []\n",
    "\n",
    "percentiles = {0: 0, 25: 16, 95: 119, 100:689}\n",
    "percentilesss = list(percentiles.keys())\n",
    "per = list(percentiles.values())\n",
    "\n",
    "for i in range(len(per) - 1):\n",
    "    item_ids_arr_popularity_groups.append(test_data_user_item_augmented.loc[(test_data_user_item_augmented['item_num_interactions'] > per[i]) & (test_data_user_item_augmented['item_num_interactions'] <= per[i + 1])]['item_id'])\n",
    "    \n",
    "distances_arr = [[] for _ in range(len(item_ids_arr_popularity_groups))]\n",
    "for item_group in range(len(item_ids_arr_popularity_groups)):\n",
    "    for k in item_ks:\n",
    "        distances_arr[item_group].append(distance_to_prototypes(item_ids_arr_popularity_groups[item_group], item_prototypes, item_embeddings, mode='top_k', k=k).mean())\n",
    "distances_arr = np.array(distances_arr)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Your data\n",
    "ks = np.arange(1, item_prototypes.shape[0], 5)\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ks, y=distances_arr[0], mode='lines', name='first 25%'))\n",
    "fig.add_trace(go.Scatter(x=ks, y=distances_arr[1], mode='lines', name='25% to 75%'))\n",
    "fig.add_trace(go.Scatter(x=ks, y=distances_arr[2], mode='lines', name='top 5% popular'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Average distance to prototypes',\n",
    "    xaxis_title='number of prototypes',\n",
    "    yaxis_title='average distance to prototypes',\n",
    "    width=500,\n",
    "    height=400,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Your data\n",
    "ks = np.arange(1, item_prototypes.shape[0], 5)\n",
    "\n",
    "# Create traces\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=ks, y=distances_arr[0], mode='lines', name='First 25%', line=dict(color='blue', width=6)))\n",
    "fig.add_trace(go.Scatter(x=ks, y=distances_arr[1], mode='lines', name='25% to 75%', line=dict(color='green', width=6)))\n",
    "fig.add_trace(go.Scatter(x=ks, y=distances_arr[2], mode='lines', name='Top 5% popular', line=dict(color='red', width=6)))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    # title='Average Distance to Prototypes',\n",
    "    xaxis_title='Number of Prototypes',\n",
    "    yaxis_title='Average Distance to Prototypes',\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    font=dict(family='Arial', size=24),\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    paper_bgcolor='rgba(0,0,0,0)',\n",
    "    xaxis=dict(showline=True, showgrid=True, linecolor='rgb(0, 0, 0)', gridcolor='rgb(50,50,50)', linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=18)),\n",
    "    yaxis=dict(showline=True, showgrid=True, linecolor='rgb(0, 0, 0)', gridcolor='rgb(50,50,50)', linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=18)),\n",
    "    legend=dict(orientation='h', yanchor='top', y=1.1, xanchor='center', x=0.5),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ks = np.arange(1, item_prototypes.shape[0], 5)\n",
    "change_model('kl')\n",
    "item_ids_arr_popularity_groups = []\n",
    "\n",
    "percentiles = {0: 0, 25: 21, 95: 265, 100:689}\n",
    "percentilesss = list(percentiles.keys())\n",
    "per = list(percentiles.values())\n",
    "\n",
    "for i in range(len(per) - 1):\n",
    "    item_ids_arr_popularity_groups.append(test_data_user_item_augmented.loc[(test_data_user_item_augmented['item_num_interactions'] > per[i]) & (test_data_user_item_augmented['item_num_interactions'] <= per[i + 1])]['item_id'])\n",
    "    \n",
    "distances_arr = [[] for _ in range(len(item_ids_arr_popularity_groups))]\n",
    "for item_group in range(len(item_ids_arr_popularity_groups)):\n",
    "    for k in item_ks:\n",
    "        distances_arr[item_group].append(distance_to_prototypes(item_ids_arr_popularity_groups[item_group], item_prototypes, item_embeddings, mode='top_k', k=k).mean())\n",
    "distances_arr = np.array(distances_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# each popular item has a prototype, and also in general they're closer to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Performance parity accross sub groups\n",
    "- Fairness in recommendation\n",
    "    - Ginicoefficient\n",
    "    - KL-divergence\n",
    "    - Difference\n",
    "    https://arxiv.org/pdf/2306.00403.pdf\n",
    "\n",
    "    \n",
    "- Discoverability Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness Evaluation Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_dic(metrics_dic):\n",
    "    metrics_dic_new = {}\n",
    "    for country, metrics in metrics_dic.items():\n",
    "        for metric, value in metrics.items():\n",
    "            # If the metric doesn't exist in the new dictionary, create it with an empty dictionary\n",
    "            if metric not in metrics_dic_new:\n",
    "                metrics_dic_new[metric] = {}\n",
    "            # Add the value for the country under the corresponding metric\n",
    "            metrics_dic_new[metric][country] = value\n",
    "    return metrics_dic_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.eval import Evaluator\n",
    "import experiment_helper\n",
    "\n",
    "def evaluate_for_user_subgroups(model, column='user_gender', subgroups=['m', 'f']):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = experiment_helper.load_data(config, is_train=False)['test_loader']\n",
    "    groups = {subgroup:test_data_user_item_augmented[test_data_user_item_augmented[column] == subgroup]['user_id'] for subgroup in subgroups}\n",
    "\n",
    "\n",
    "    # print(groups)\n",
    "    group_losses = {}\n",
    "    group_evals = {}\n",
    "\n",
    "    for group_name, subgroup in groups.items():\n",
    "        \n",
    "        print('group name, subgroup', group_name, len(subgroup))\n",
    "        \n",
    "        test_loss = 0\n",
    "        eval = Evaluator(0)\n",
    "        \n",
    "        # group name, subgroup US 687\n",
    "        # (687,) torch.Size([256])\n",
    "        for u_idxs, i_idxs, labels in test_loader:\n",
    "            # in each batch, keep only the u_idxs that are in the subgroup\n",
    "            mask = pd.Series(u_idxs).isin(subgroup)\n",
    "            \n",
    "            print(subgroup.shape, u_idxs.shape)\n",
    "            break\n",
    "            u_idxs, i_idxs, labels = u_idxs[mask], i_idxs[mask], labels[mask]\n",
    "            if len(u_idxs) == 0:\n",
    "                continue\n",
    "            eval.n_users += len(u_idxs)\n",
    "            out = model(u_idxs, i_idxs).detach()\n",
    "            test_loss += model.loss_func(out, labels).item()\n",
    "            out = nn.Sigmoid()(out)\n",
    "            out = out.to('cpu')\n",
    "\n",
    "            eval.eval_batch(out, group_name)\n",
    "\n",
    "        group_losses[group_name] = test_loss\n",
    "        group_evals[group_name] = eval\n",
    "\n",
    "    return group_losses, group_evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.eval import Evaluator\n",
    "import experiment_helper\n",
    "\n",
    "def evaluate_for_item_subgroups(model, column='artist_gender', subgroups=['Male', 'Female']): # artist_country\n",
    "\n",
    "    test_loader = experiment_helper.load_data(config, is_train=False)['test_loader']\n",
    "    groups = {subgroup:test_data_user_item_augmented[test_data_user_item_augmented[column] == subgroup]['item_id'] for subgroup in subgroups}\n",
    "\n",
    "    group_losses = {}\n",
    "    group_evals = {}\n",
    "                \n",
    "    for u_idxs, i_idxs, labels in test_loader:    \n",
    "        test_loss = 0\n",
    "        eval = Evaluator(u_idxs)\n",
    "        for group_name, subgroup in groups.items():                \n",
    "            out = model(u_idxs, i_idxs).detach()\n",
    "            test_loss += model.loss_func(out, labels).item()\n",
    "            out = nn.Sigmoid()(out)\n",
    "            out = out.to('cpu')\n",
    "\n",
    "            i_idxs_flat = i_idxs.flatten()\n",
    "            i_idxs_in_subgroup = pd.Series(i_idxs_flat).isin(subgroup).values.reshape(i_idxs.shape)\n",
    "            # TODO MONDAY!!!!\n",
    "\n",
    "            eval.eval_batch(out, group_name)\n",
    "\n",
    "            group_losses[group_name] = test_loss\n",
    "            group_evals[group_name] = eval\n",
    "            \n",
    "            if len(i_idxs_in_subgroup) == 0:\n",
    "                print('no candidates')\n",
    "                continue\n",
    "\n",
    "            # calculate the rankings for the indices that are in i_idxs_in_subgroup            \n",
    "    return group_losses, group_evals\n",
    "\n",
    "\n",
    "group_losses, group_evals = evaluate_for_item_subgroups(model, column='artist_gender', subgroups=['Male', 'Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_flat[800:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the rank of the first item? in out_flat\n",
    "ar = -out_flat[800:900]\n",
    "ar.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(out_flat).isin(subg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, m = evals['Female'], evals['Male']\n",
    "f.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_for_user_subgroups(model, column='user_gender', subgroups=['m', 'f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_column = 'user_country'\n",
    "feature_subgroups = ['US', 'UK', 'IR', 'IT', 'CA', 'UA']\n",
    "losses, metrics = evaluate_for_user_subgroups(model, column=feature_column, subgroups=feature_subgroups)\n",
    "# metrics_dic = {}\n",
    "# for subgroup in feature_subgroups:\n",
    "#     metrics_dic[subgroup] = metrics[subgroup].get_results()\n",
    "\n",
    "# metrics_dic = swap_dic(metrics_dic)\n",
    "\n",
    "# countries = metrics_dic.keys()\n",
    "# metrics = list(metrics_dic['hit_ratio@10'].keys())\n",
    "\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# keys = list(metrics_dic.keys())\n",
    "# metrics = list(metrics_dic[keys[0]].keys())\n",
    "\n",
    "# bar_width = 0.07\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# for i in range(len(metrics)):\n",
    "#     metric_values = [metrics_dic[key][metrics[i]] for key in keys]\n",
    "#     ax.bar([x + i * bar_width for x in range(len(keys))], metric_values, bar_width, label=metrics[i])\n",
    "\n",
    "# ax.set_xlabel('Metrics')\n",
    "# ax.set_ylabel('Values')\n",
    "# ax.set_title(f'Metrics for Different Subgroups, trained with k = {model_k}')\n",
    "# ax.set_xticks([x + 0.2 for x in range(len(keys))])\n",
    "\n",
    "# plt.xticks(fontsize=4)\n",
    "# ax.set_xticklabels(keys)\n",
    "# ax.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, metrics = evaluate_for_user_subgroups(model)\n",
    "metrics_dic = {}\n",
    "for subgroup in ['m', 'f']:\n",
    "    metrics_dic[subgroup] = metrics[subgroup].get_results()\n",
    "    \n",
    "metrics_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluate_user_subgroups(metrics_dic):\n",
    "    \n",
    "    metrics_dic = swap_dic(metrics_dic)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    keys = list(metrics_dic.keys())\n",
    "    metrics = list(metrics_dic[keys[0]].keys())\n",
    "    \n",
    "    bar_width = 0.07\n",
    "    num_metrics = len(metrics)\n",
    "    index = range(len(keys))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(num_metrics):\n",
    "        metric_values = [metrics_dic[key][metrics[i]] for key in keys]\n",
    "        ax.bar([x + i * bar_width for x in index], metric_values, bar_width, label=metrics[i])\n",
    "\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Metrics')\n",
    "    ax.set_title(f'Metrics for Different Keys, trained with k = {model_k}')\n",
    "    plt.xticks(fontsize=4)\n",
    "    ax.set_xticks([x + 0.1 for x in index])\n",
    "    ax.set_xticklabels(keys)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluate_user_subgroups(metrics_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prototype_model = model.user_feature_extractor.model_1\n",
    "item_prototype_model = model.item_feature_extractor.model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.explanations_utils import weight_visualization, get_top_k_items, tsne_plot\n",
    "\n",
    "user_embeddings = user_prototype_model.embedding_ext.embedding_layer.weight[:]\n",
    "user_prototypes = user_prototype_model.prototypes\n",
    "tsne_plot(objects=user_embeddings.detach().numpy(), prototypes=user_prototypes.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.explanations_utils import weight_visualization, get_top_k_items, tsne_plot\n",
    "\n",
    "\n",
    "user_indices = test_data_user_item_augmented['user_id'].values # sample the users you want to plot\n",
    "# user_indices = user_indices[:100]\n",
    "\n",
    "attributes_dict = {'user_gender':[], 'user_age':[], 'user_country':[],'user_num_interactions':[]}\n",
    "test_data_user_item_augmented_arr = test_data_user_item_augmented[test_data_user_item_augmented['user_id'].isin(user_indices)][list(attributes_dict.keys())].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices\n",
    "user_indices = user_indices.astype(int)\n",
    "for i, key in enumerate(attributes_dict.keys()):\n",
    "    print(i, key)\n",
    "    attributes_dict[key] = test_data_user_item_augmented_arr[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = user_prototype_model.embedding_ext.embedding_layer.weight[user_indices]\n",
    "user_prototypes = user_prototype_model.prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User based\n",
    "### 1. gender\n",
    "### 2. age\n",
    "### 3. creation year\n",
    "### 4 num interactions\n",
    "### 5. country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(objects=user_embeddings.detach().numpy(), prototypes=user_prototypes.detach().numpy(), labels=attributes_dict['user_gender'], title='user_gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(objects=user_embeddings.detach().numpy(), prototypes=user_prototypes.detach().numpy(), sizes=attributes_dict['user_age'].astype(float)**2, title='User Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(objects=user_embeddings.detach().numpy(), prototypes=user_prototypes.detach().numpy(), sizes=(attributes_dict['creation_year'] - 2000).astype(float)**2, title='User Account Creation Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(objects=user_embeddings.detach().numpy(), prototypes=user_prototypes.detach().numpy(), sizes=attributes_dict['user_num_interactions'].astype(float), title='User Interactions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = test_data_user_item_augmented.groupby(['user_country']).size().sort_values(ascending=False)\n",
    "chosen_countries = list(country_counts[:3].index) + list(country_counts[40:43].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the rows with the chosen countries\n",
    "test_data_user_item_augmented_chosen_countries = test_data_user_item_augmented[test_data_user_item_augmented['user_country'].isin(chosen_countries)]\n",
    "user_countries = test_data_user_item_augmented_chosen_countries['user_country'].values\n",
    "\n",
    "user_indices = test_data_user_item_augmented_chosen_countries['user_id'].values\n",
    "user_embeddings = user_prototype_model.embedding_ext.embedding_layer.weight[user_indices]\n",
    "\n",
    "tsne_plot(objects=user_embeddings.detach().numpy(), prototypes=user_prototypes.detach().numpy(), labels=user_countries, title='User Countries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracks based\n",
    "### 1. gender\n",
    "### 2. num_interactions\n",
    "### 3. country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_data_user_item_augmented['artist_gender'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.explanations_utils import weight_visualization, get_top_k_items, tsne_plot\n",
    "\n",
    "item_indices = test_data_user_item_augmented['item_id'].values.astype(int) # sample the users you want to plot\n",
    "# item_indices = item_indices[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_dict = {'artist_gender':[], 'artist_country':[], 'item_num_interactions':[]}\n",
    "test_data_user_item_augmented_arr = test_data_user_item_augmented[test_data_user_item_augmented['item_id'].isin(item_indices)][list(attributes_dict.keys())].values\n",
    "for i, key in enumerate(attributes_dict.keys()):\n",
    "    print(i, key)\n",
    "    attributes_dict[key] = test_data_user_item_augmented_arr[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = item_prototype_model.embedding_ext.embedding_layer.weight[item_indices]\n",
    "item_prototypes = item_prototype_model.prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(item_embeddings.detach().numpy(), item_prototypes.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented[['item_id', 'item_num_interactions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_model('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def tsne_plot(objects: np.ndarray, prototypes: np.ndarray, object_legend_text: str = 'Object', labels: np.ndarray = ['#74add1'], sizes: np.ndarray = [30], perplexity: int = 5,\n",
    "              path_save_fig: str = None, title: str = 'TSNE plot for Object (user/item) Embeddings', us_indices=None, jp_indices=None):\n",
    "\n",
    "    tsne = TSNE(perplexity=perplexity, metric='cosine', init='pca', learning_rate='auto', random_state=42)\n",
    "    print('start')\n",
    "    \n",
    "    tsne_results = tsne.fit_transform(np.vstack([prototypes, objects]))\n",
    "    \n",
    "    print('finish')\n",
    "    tsne_protos = tsne_results[:len(prototypes)]\n",
    "    tsne_embeds = tsne_results[len(prototypes):]\n",
    "\n",
    "    plt.figure(figsize=(10, 10), dpi=100)\n",
    "    \n",
    "    print('figs')\n",
    "    # hsv\n",
    "    cmap = plt.get_cmap('hsv')\n",
    "    if len(labels) != 1:\n",
    "        for i, label in enumerate(np.unique(labels)):\n",
    "            plt.scatter(tsne_embeds[labels == label, 0], tsne_embeds[labels == label, 1], s=sizes, alpha=0.6, c=cmap(i / len(np.unique(labels))), label=label)\n",
    "    else:\n",
    "        plt.scatter(tsne_embeds[:, 0], tsne_embeds[:, 1], s=sizes, alpha=0.4, c=labels, label=object_legend_text, marker='o')\n",
    "    plt.scatter(tsne_protos[:, 0], tsne_protos[:, 1], s=200, c='#d73027', alpha=0.5, label='Prototypes', marker='^')\n",
    "\n",
    "\n",
    "    if us_indices is not None:\n",
    "        chosen_points_US = tsne_embeds[us_indices]\n",
    "        plt.scatter(chosen_points_US[:, 0], chosen_points_US[:, 1], s=220, c='blue', alpha=0.5, label='US Item', marker='x', linewidths=4)\n",
    "    \n",
    "    if jp_indices is not None:\n",
    "        chosen_points_JP = tsne_embeds[jp_indices]\n",
    "        plt.scatter(chosen_points_JP[:, 0], chosen_points_JP[:, 1], s=220, c='green', alpha=0.5, label='NL Item', marker='x', linewidths=4)\n",
    "\n",
    "\n",
    "    print('done')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"upper left\", prop={'size': 13})\n",
    "    if path_save_fig:\n",
    "        plt.savefig(path_save_fig, format='pdf')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_track_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train_dataset_track_augmented[['item_id', 'item_num_interactions', 'country']]\n",
    "df.drop_duplicates(subset=['item_id'], inplace=True)\n",
    "item_sizes = np.zeros(item_embeddings.shape[0])\n",
    "len(item_sizes), item_embeddings.shape\n",
    "for item_id in range(len(item_sizes)):\n",
    "    try:\n",
    "        item_sizes[item_id] = df[df['item_id'] == item_id]['item_num_interactions'].astype(float)\n",
    "    except:\n",
    "        item_sizes[item_id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_id in range(len(item_sizes)):\n",
    "    try:\n",
    "        item_sizes[item_id] = df[df['item_id'] == item_id]['item_num_interactions'].astype(float)\n",
    "    except:\n",
    "        item_sizes[item_id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attributes_dict = {'country':[], 'item_num_interactions':[]}\n",
    "# train_data = train_dataset_track_augmented[train_dataset_track_augmented['item_id'].isin(item_indices)][list(attributes_dict.keys())].values\n",
    "# for i, key in enumerate(attributes_dict.keys()):\n",
    "    # print(train_dataset_track_augmented[:, i])\n",
    "    # attributes_dict[key] = train_dataset_track_augmented[:, i]\n",
    "change_model('kl')\n",
    "n = 4000\n",
    "\n",
    "us_item_indices = train_dataset_track_augmented.loc[train_dataset_track_augmented.country == 'US']['item_id'].unique()\n",
    "us_item_indices = us_item_indices[us_item_indices < n][15:20]\n",
    "\n",
    "jp_item_indices = train_dataset_track_augmented.loc[train_dataset_track_augmented.country == 'NL']['item_id'].unique()\n",
    "jp_item_indices = jp_item_indices[jp_item_indices < n][:5]\n",
    "\n",
    "print('sss')\n",
    "tsne_plot(item_embeddings.detach().numpy()[:n], item_prototypes.detach().numpy(), sizes=item_sizes[:n], title='Item Interactions', us_indices=us_item_indices, jp_indices=jp_item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -len(item_embeddings)//10\n",
    "n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_track_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_dict = {'country':[], 'item_num_interactions':[]}\n",
    "train_data = train_dataset_track_augmented[train_dataset_track_augmented['item_id'].isin(item_indices)][list(attributes_dict.keys())].values\n",
    "for i, key in enumerate(attributes_dict.keys()):\n",
    "    print(train_dataset_track_augmented[:, i])\n",
    "    attributes_dict[key] = train_dataset_track_augmented[:, i]\n",
    "\n",
    "us_item_indices = train_dataset_track_augmented.loc[train_dataset_track_augmented.artist_country == 'US']['item_id'].values\n",
    "jp_item_indices = train_dataset_track_augmented.loc[train_dataset_track_augmented.artist_country == 'JP']['item_id'].values\n",
    "\n",
    "\n",
    "print('sss')\n",
    "\n",
    "n = -5500\n",
    "tsne_plot(item_embeddings[n:].detach().numpy(), item_prototypes.detach().numpy(), sizes=attributes_dict['item_num_interactions'].astype(float)[n:], title='Item Interactions', us_indices=us_item_indices, jp_indices=jp_item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -len(item_embeddings)//10\n",
    "tsne_plot(item_embeddings[-5000:].detach().numpy(), item_prototypes.detach().numpy(), sizes=attributes_dict['item_num_interactions'].astype(float)[-5000:], title='Item Interactions')#, us_indices=us_item_indices, jp_indices=jp_item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(item_embeddings.detach().numpy(), item_prototypes.detach().numpy(), sizes=attributes_dict['item_num_interactions'].astype(float), title='Item Interactions', us_indices=us_item_indices, jp_indices=jp_item_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data, only Female and Male genders\n",
    "item_indices = test_data_user_item_augmented.loc[test_data_user_item_augmented.artist_gender.isin(['Male', 'Female'])]['item_id'].values\n",
    "item_embeddings = item_prototype_model.embedding_ext.embedding_layer.weight[item_indices]\n",
    "item_prototypes = item_prototype_model.prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_dict['artist_gender'] = test_data_user_item_augmented[test_data_user_item_augmented['item_id'].isin(item_indices)]['artist_gender'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(objects=item_embeddings.detach().numpy(), prototypes=item_prototypes.detach().numpy(), labels=attributes_dict['artist_gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_counts = test_data_user_item_augmented.groupby(['user_country']).size().sort_values(ascending=False)\n",
    "chosen_countries = list(country_counts[3:15].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_user_item_augmented.loc[test_data_user_item_augmented.artist_country.isin(chosen_countries)].groupby('artist_country').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data, only Female and Male genders\n",
    "item_indices = test_data_user_item_augmented.loc[test_data_user_item_augmented.artist_country.isin(chosen_countries)]['item_id'].values\n",
    "item_embeddings = item_prototype_model.embedding_ext.embedding_layer.weight[item_indices]\n",
    "\n",
    "item_prototypes = item_prototype_model.prototypes\n",
    "\n",
    "attributes_dict['artist_country'] = test_data_user_item_augmented[test_data_user_item_augmented['item_id'].isin(item_indices)]['artist_country'].values\n",
    "\n",
    "tsne_plot(objects=item_embeddings.detach().numpy(), prototypes=item_prototypes.detach().numpy(), labels=attributes_dict['artist_country'], title='TSNE FOR more COMMON COUNTRIES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from rec_sys.tester import Tester\n",
    "from experiment_helper import load_data\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from utilities.eval import Evaluator\n",
    "import experiment_helper\n",
    "\n",
    "\n",
    "\n",
    "def calc_fairness_metrics(model_outputs):\n",
    "    under_countries = ['BR', 'NL', 'PL']\n",
    "    countries = [model_outputs[0][key] for key in under_countries]\n",
    "    ranking_mean_under = np.mean([np.mean(country_avg_ranking) for country_avg_ranking in countries])\n",
    "    \n",
    "    over_countries = ['US', 'GB']\n",
    "    countries = [model_outputs[0][key] for key in over_countries]\n",
    "    ranking_mean_over = np.mean([np.mean(country_avg_ranking) for country_avg_ranking in countries])\n",
    "    \n",
    "    gender = [model_outputs[1][key] for key in ['Female']]\n",
    "    ranking_mean_female = np.mean([np.mean(country_avg_ranking) for country_avg_ranking in gender])\n",
    "\n",
    "    gender = [model_outputs[1][key] for key in ['Male']]\n",
    "    ranking_mean_male = np.mean([np.mean(country_avg_ranking) for country_avg_ranking in gender])\n",
    "\n",
    "    rank_tuples = model_outputs[2]\n",
    "    lt_normal_perc = train_dataset_track_augmented['item_num_interactions'].quantile(0.2)\n",
    "    lt_log_perc = pd.Series(np.log(train_dataset_track_augmented['item_num_interactions'])).quantile(0.2)**2\n",
    "\n",
    "    lt_array = np.array([(x, y < lt_normal_perc, y < lt_log_perc)  for  (id, x, y) in rank_tuples])\n",
    "    ranking_mean_for_normal_lt = np.mean(lt_array[lt_array[:, 1], 0])\n",
    "    ranking_mean_for_log_lt = np.mean(lt_array[lt_array[:, 2], 0])\n",
    "\n",
    "\n",
    "    lt_array = np.array([(id, y < lt_normal_perc, y < lt_log_perc)  for  (id, x, y) in rank_tuples])\n",
    "    unique_values_normal = len(np.unique(lt_array[lt_array[:, 1], 0]))\n",
    "    unique_values_log = len(np.unique(lt_array[lt_array[:, 2], 0]))\n",
    "    \n",
    "        \n",
    "    fairness_metrics = {'under_countries_avg_rank': ranking_mean_under, 'over_countries_avg_rank': ranking_mean_over, 'mean_rank_female': ranking_mean_female, 'mean_rank_male': ranking_mean_male, 'mean_rank_lt_normal': ranking_mean_for_normal_lt, 'mean_rank_lt_log':ranking_mean_for_log_lt, 'unique_values_normal': unique_values_normal, 'unique_values_log': unique_values_log}\n",
    "    return fairness_metrics\n",
    "\n",
    "def get_outputs_for_eval(model):   \n",
    "    outs_array_items = []\n",
    "    outs_array_users = []\n",
    "\n",
    "    for u_idxs, i_idxs, labels in test_loader:    \n",
    "        test_loss = 0\n",
    "        eval = Evaluator(u_idxs)\n",
    "        out = model(u_idxs, i_idxs).detach()\n",
    "        test_loss += model.loss_func(out, labels).item()\n",
    "        out = nn.Sigmoid()(out)\n",
    "        out = out.to('cpu')\n",
    "        \n",
    "        outs_array_items.append(np.array(torch.stack((i_idxs, labels, out), dim=2)))\n",
    "        outs_array_users.append(np.array(u_idxs))\n",
    "    return outs_array_items, outs_array_users\n",
    "\n",
    "\n",
    "\n",
    "def get_processed_outputs(outs_array_items):\n",
    "    pop_rank_tuples = []\n",
    "    countries = {'US': [], 'GB': [], 'BR': [], 'NL': [], 'PL': [], 'CA': [], 'IR': []}  # AT: austria , JP: japan, CA: canada, US: us, GB: gb, FR: france, RU: russia, IT: Italy\n",
    "    genders = {'Male':[], 'Female':[]}\n",
    "    for epoch_recom_list in outs_array_items:\n",
    "        for users_recom_list in epoch_recom_list:\n",
    "            sorted_indices = np.argsort(users_recom_list[:, 2])[::-1]\n",
    "            ranked_outputs = np.empty_like(users_recom_list[:, 2])\n",
    "            ranked_outputs[sorted_indices] = np.arange(0, len(users_recom_list[:, 2]))            \n",
    "            users_recom_list = np.column_stack((users_recom_list, ranked_outputs.reshape(-1, 1)))\n",
    "                        \n",
    "            for item in users_recom_list:\n",
    "                item_id = int(item[0])\n",
    "                _ = item[1]\n",
    "                item_output = item[2]\n",
    "                item_rank = int(item[3])\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    item_pop = train_dataset_track_augmented[train_dataset_track_augmented['item_id'] == item_id]['item_num_interactions'].iloc[0]                    \n",
    "                    pop_rank_tuples.append((item_id, item_pop, item_rank))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    item_country = train_dataset_track_augmented[train_dataset_track_augmented['item_id'] == item_id]['country'].iloc[0]\n",
    "                    if item_country in list(countries.keys()):\n",
    "                        countries[item_country].append(item_rank)\n",
    "                except:\n",
    "                    pass\n",
    "                                    \n",
    "                try:\n",
    "                    item_gender = train_dataset_track_augmented[train_dataset_track_augmented['item_id'] == item_id]['gender'].iloc[0]\n",
    "                    if item_gender in list(genders.keys()):\n",
    "                        genders[item_gender].append(item_rank)\n",
    "                except:\n",
    "                    pass\n",
    "        break\n",
    "    return countries, genders, pop_rank_tuples\n",
    "\n",
    "\n",
    "dataset_path = 'data/lfm2b-1mon'\n",
    "users = pd.read_csv(os.path.join(dataset_path, 'users.tsv'), sep='\\t')\n",
    "users.rename(columns={'user_id': 'old_user_id'}, inplace=True)\n",
    "test_dataset_path = dataset_path + '/listening_history_test.csv'\n",
    "test_data = pd.read_csv(test_dataset_path)\n",
    "test_data_user_augmented = test_data.merge(users, on='old_user_id')\n",
    "tracks = pd.read_csv(os.path.join(dataset_path, 'tracks_augmented.csv'))\n",
    "tracks.rename(columns={'track_id': 'old_item_id'}, inplace=True)\n",
    "test_data_user_item_augmented = test_data_user_augmented.merge(tracks, on='old_item_id')\n",
    "test_data_user_item_augmented.rename(columns={'country_x': 'user_country', 'age':'user_age', 'gender_x': 'user_gender', 'artist': 'artist_name', 'track':'track_name', 'gender_y':'artist_gender', 'country_y':'artist_country'}, inplace=True)\n",
    "train_dataset = pd.read_csv(os.path.join(dataset_path, 'listening_history_train.csv'))\n",
    "test_data_user_item_augmented['user_num_interactions'] = train_dataset.groupby('user_id').size()[test_data_user_item_augmented['user_id']].values\n",
    "test_data_user_item_augmented['item_num_interactions'] = train_dataset.groupby('item_id').size()[test_data_user_item_augmented['item_id']].values\n",
    "train_dataset_track_augmented = train_dataset.merge(tracks, on='old_item_id')\n",
    "train_dataset_track_augmented['user_num_interactions'] = train_dataset.groupby('user_id').size()[train_dataset_track_augmented['user_id']].values\n",
    "train_dataset_track_augmented['item_num_interactions'] = train_dataset.groupby('item_id').size()[train_dataset_track_augmented['item_id']].values\n",
    "print(train_dataset_track_augmented.head())\n",
    "\n",
    "# TODO ADD args\n",
    "\n",
    "model_path = \"\"\"ray_results/user_item_proto_lfm2b-1mon_cn_38210573_2024-4-5_15-12-24.486415/user_item_proto_lfm2b-1mon_cn_38210573_36e2aa7f_6_batch_size=313,data_path\"\"\"\n",
    "path_to_experiment = model_path\n",
    "config_path = os.path.join(path_to_experiment, 'params.json')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "config['device'] = 'cpu'\n",
    "config_dict = config.copy()\n",
    "config = argparse.Namespace(**config)\n",
    "data_loaders_dict = load_data(config, is_train=False)\n",
    "\n",
    "model_load_path = os.path.join(model_path, 'checkpoint_000000/best_model.pth')\n",
    "tester = Tester(data_loaders_dict['test_loader'], config, model_load_path)\n",
    "model = tester.model\n",
    "\n",
    "test_loader = experiment_helper.load_data(config, is_train=False)['test_loader']\n",
    "\n",
    "outs_model_items, outs_base_users = get_outputs_for_eval(model)\n",
    "model_outputs_processed = get_processed_outputs(outs_model_items)\n",
    "\n",
    "metrics = calc_fairness_metrics(model_outputs_processed)\n",
    "\n",
    "\n",
    "print(metrics)\n",
    "# TODO: write to file\n",
    "file_name = 'jobid'\n",
    "LOGS_PATH = f\"logs/FAIRNESS_METRICS/{0}\"\n",
    "with open(LOGS_PATH.format(file_name), \"w\") as json_file:\n",
    "    json.dump(metrics, json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = calc_fairness_metrics(model_outputs_processed)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
